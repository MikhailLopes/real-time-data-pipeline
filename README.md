Project on building a real pipeline

# ğŸ“¡ Real-Time Data Pipeline with Kafka

## ğŸ“Œ DescriÃ§Ã£o
Este projeto consiste na construÃ§Ã£o de um **pipeline de dados em tempo real** utilizando **Apache Kafka** como sistema de mensageria. O objetivo Ã© processar eventos de logs gerados por aplicaÃ§Ãµes e armazenÃ¡-los para anÃ¡lise futura.

## ğŸ—ï¸ Arquitetura Inicial
1. **Produtor (API ou Script)** â†’ Envia logs para o Kafka.
2. **Kafka (Mensageria)** â†’ Encaminha os eventos para os consumidores.
3. **Consumidor (Python/Spark)** â†’ Processa os logs recebidos.
4. **Banco de Dados (PostgreSQL/MongoDB)** â†’ Armazena os logs processados para consulta e anÃ¡lise.

## ğŸ”§ Tecnologias Utilizadas
- **Apache Kafka** â€“ Mensageria para comunicaÃ§Ã£o assÃ­ncrona.
- **Python** â€“ Para produtores e consumidores de eventos.
- **Docker** â€“ Para facilitar a implantaÃ§Ã£o do Kafka.
- **PostgreSQL / MongoDB** â€“ Para armazenamento dos dados processados.

## ğŸš€ Como Executar o Projeto
### 1ï¸âƒ£ Clonar o repositÃ³rio:
```sh
 git clone https://github.com/seu-usuario/real-time-data-pipeline.git
 cd real-time-data-pipeline
```
### 2ï¸âƒ£ Subir o Kafka via Docker:
```sh
 docker-compose up -d
```
### 3ï¸âƒ£ Executar o produtor de eventos:
```sh
 python producer/producer.py
```
### 4ï¸âƒ£ Executar o consumidor de eventos:
```sh
 python consumer/consumer.py
```

## ğŸ“‚ Estrutura do Projeto
```
real-time-data-pipeline/
â”‚â”€â”€ producer/         # CÃ³digo do produtor de eventos
â”‚â”€â”€ consumer/         # CÃ³digo do consumidor de eventos
â”‚â”€â”€ database/         # Scripts para banco de dados
â”‚â”€â”€ docs/             # DocumentaÃ§Ã£o do projeto
â”‚â”€â”€ docker-compose.yml  # ConfiguraÃ§Ã£o do Kafka
â”‚â”€â”€ README.md         # DocumentaÃ§Ã£o principal
```


